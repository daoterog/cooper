{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import cooper\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Problem\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\underset{x}{min} \\quad x^Tx \\\\\n",
    "s.t. \\quad Ax = b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "With $x \\in \\mathbb{R}^n, \\, A \\in \\mathbb{R}^{p \\times n}, \\, b \\in \\mathbb{R}^{p}$. In this case the p will correspond to the number of equations and n to the number of variables.\n",
    "\n",
    "The Lagrangian is:\n",
    "\n",
    "$$\n",
    "L(x, v) = x^Tx + v^T(Ax - b)\n",
    "$$\n",
    "\n",
    "And its domain is $\\mathbb{R}^n \\times \\mathbb{R}^p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vars = 20         # n\n",
    "n_eq = 2            # p\n",
    "scaling_value = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly build matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 20]), torch.Size([2, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(size=(n_eq,n_vars))*scaling_value\n",
    "b = torch.rand(size=(n_eq,1))*scaling_value\n",
    "\n",
    "A.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for feasibility\n",
    "\n",
    "- Unique Solution:\n",
    "\n",
    "The system has unique solution when the equations are all consistent and the number of equations are equal to the number of rows. Simply put, if the non-augmented matrix has a nonzero determinant. This problem is not interesting for us because it will only test the algorithms ability to arrive to the solution of the system.\n",
    "\n",
    "- Infinetely many solutions\n",
    "\n",
    "The number of equations is less than the number of variables. This is the problem that we are interested in, since we will want to find the variables that minimize the squared norm and satisfy the linear system.\n",
    "\n",
    "- No solutions\n",
    "\n",
    " If the linear system is inconsistent, this is, the rank of the non-augmented matrix is not equal to the rank of the augmented matrix. Generally this happens when we have more equations than variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented matrix rank: 2\n",
      "Matrix rank: 2\n",
      "It has infinite solutions\n"
     ]
    }
   ],
   "source": [
    "if n_eq == n_vars:\n",
    "    det = torch.det(A)\n",
    "    print(f'A is a square matrix with determinant {det}')\n",
    "    assert det.item() != 0, 'A is not invertible'\n",
    "\n",
    "# Check for consistency of the formulation\n",
    "augmented_matrix = torch.cat([A, b], dim=1)\n",
    "rank_A = torch.linalg.matrix_rank(A)\n",
    "rank_aug = torch.linalg.matrix_rank(augmented_matrix)\n",
    "\n",
    "print(f'Augmented matrix rank: {rank_aug}\\nMatrix rank: {rank_A}')\n",
    "\n",
    "assert rank_aug == rank_A, \"A rank is different from A|b rank\"\n",
    "\n",
    "print('It has infinite solutions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMP definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastSquares(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, A: torch.tensor, b: torch.tensor) -> None:\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        super().__init__(is_constrained=True)\n",
    "\n",
    "    def closure(self, x: torch.tensor) -> cooper.CMPState:\n",
    "\n",
    "        loss = torch.matmul(torch.t(x), x)\n",
    "\n",
    "        eq_defect = torch.matmul(self.A, x) - self.b\n",
    "\n",
    "        return cooper.CMPState(\n",
    "            loss=loss, eq_defect=eq_defect\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the problem and the lagrangian formulation associated with it.\n",
    "cmp = LeastSquares(A, b)\n",
    "formulation = cooper.LagrangianFormulation(cmp)\n",
    "\n",
    "# Instantiate the variable to optimize\n",
    "x = torch.nn.Parameter(torch.rand(n_vars, 1))\n",
    "\n",
    "# Instantiate the optimizers\n",
    "primal_optimizer = torch.optim.SGD([x], lr=3e-2, momentum=0.7)\n",
    "dual_optimizer = cooper.optim.partial_optimizer(\n",
    "    torch.optim.SGD, lr=9e-3, momentum=0.7\n",
    "    )\n",
    "\n",
    "# Instatiate the constrained optimizer\n",
    "coop = cooper.ConstrainedOptimizer(\n",
    "    formulation=formulation,\n",
    "    primal_optimizer=primal_optimizer,\n",
    "    dual_optimizer=dual_optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMPState(loss=tensor([[0.0716]], grad_fn=<MmBackward0>), ineq_defect=None, eq_defect=tensor([[5.9605e-08],\n",
      "        [1.7881e-07]], grad_fn=<SubBackward0>), proxy_ineq_defect=None, proxy_eq_defect=None, misc=None)\n"
     ]
    }
   ],
   "source": [
    "state_history = cooper.StateLogger(save_metrics=[\"loss\", \"eq_defect\", \"eq_multipliers\"])\n",
    "\n",
    "for iter_num in range(5000):\n",
    "\n",
    "    coop.zero_grad()\n",
    "    lagrangian = formulation.composite_objective(cmp.closure, x)\n",
    "    formulation.custom_backward(lagrangian)\n",
    "    coop.step(cmp.closure, x)\n",
    "\n",
    "    # Store optimization metrics at each step\n",
    "    partial_dict = {\"params\": copy.deepcopy(x.data)}\n",
    "    state_history.store_metrics(formulation, iter_num, partial_dict)\n",
    "\n",
    "print(cmp.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17904\\3170168115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack_stored_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0max0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"iters\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eq_multipliers\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'state_history' is not defined"
     ]
    }
   ],
   "source": [
    "all_metrics = state_history.unpack_stored_metrics()\n",
    "\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(nrows=1, ncols=3, sharex=True, figsize=(15, 3))\n",
    "\n",
    "ax0.plot(all_metrics[\"iters\"], np.stack(all_metrics[\"eq_multipliers\"]).squeeze())\n",
    "ax0.set_title(\"Multipliers\")\n",
    "\n",
    "ax1.plot(all_metrics[\"iters\"], np.stack(all_metrics[\"eq_defect\"]).squeeze(), alpha=0.6)\n",
    "ax1.axhline(0.0, c=\"gray\", alpha=0.2)\n",
    "ax1.set_title(\"Defects\")\n",
    "\n",
    "ax2.plot(all_metrics[\"iters\"], all_metrics[\"loss\"])\n",
    "ax2.set_title(\"Objective\")\n",
    "\n",
    "[_.semilogx() for _ in (ax0, ax1, ax2)]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation\n",
    "\n",
    "We can encapsulate the whole process in a function that may allows us to experiment with different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_cmp(\n",
    "    n_eq: int,\n",
    "    n_vars: int,\n",
    "    scaling_value: float,\n",
    "    primal_lr: float,\n",
    "    dual_lr: float,\n",
    "    n_iter: int,\n",
    "    primal_optimizer_type: str,\n",
    "    dual_optimizer_type: str\n",
    ") -> None:\n",
    "\n",
    "    \"\"\"Solves the constrained minimization problem with the given parameters.\"\"\"\n",
    "\n",
    "    # Randomly generate the matrices\n",
    "    A = torch.rand(size=(n_eq,n_vars))*scaling_value\n",
    "    b = torch.rand(size=(n_eq,1))*scaling_value\n",
    "\n",
    "    print(f'A matrix shape: {A.shape}\\nb vector shape: {b.shape}')\n",
    "\n",
    "    # Check for consistency of the formulation\n",
    "    if n_eq == n_vars:\n",
    "        det = torch.det(A)\n",
    "        print(f'A is a square matrix with determinant {det}')\n",
    "        assert det.item() != 0, 'A is not invertible'\n",
    "        print('It has unique solution.')\n",
    "\n",
    "    augmented_matrix = torch.cat([A, b], dim=1)\n",
    "    rank_A = torch.linalg.matrix_rank(A)\n",
    "    rank_aug = torch.linalg.matrix_rank(augmented_matrix)\n",
    "\n",
    "    print(f'Augmented matrix rank: {rank_aug}\\nMatrix rank: {rank_A}')\n",
    "    assert rank_aug == rank_A, \"A rank is different from A|b rank\"\n",
    "    print('It has solution.')\n",
    "\n",
    "    # Instantiate the problem and the lagrangian formulation associated with it.\n",
    "    cmp = LeastSquares(A, b)\n",
    "    formulation = cooper.LagrangianFormulation(cmp)\n",
    "\n",
    "    # Instantiate the variable to optimize\n",
    "    x = torch.nn.Parameter(torch.rand(n_vars, 1))\n",
    "\n",
    "    # Instantiate the optimizers\n",
    "    if primal_optimizer_type == \"SGD\":\n",
    "        primal_optimizer = torch.optim.SGD([x], lr=3e-2, momentum=0.7)\n",
    "    else:\n",
    "        primal_optimizer = cooper.optim.ExtraSGD([x], lr=3e-2, momentum=0.7)\n",
    "\n",
    "    if dual_optimizer_type == \"SGD\":\n",
    "        dual_optimizer = cooper.optim.partial_optimizer(\n",
    "            torch.optim.SGD, lr=9e-3, momentum=0.7\n",
    "        )\n",
    "    else:\n",
    "        dual_optimizer = cooper.optim.partial_optimizer(\n",
    "            cooper.optim.ExtraSGD, lr=9e-3, momentum=0.7\n",
    "        )\n",
    "\n",
    "    # Instatiate the constrained optimizer\n",
    "    coop = cooper.ConstrainedOptimizer(\n",
    "        formulation=formulation,\n",
    "        primal_optimizer=primal_optimizer,\n",
    "        dual_optimizer=dual_optimizer,\n",
    "    )\n",
    "\n",
    "    state_history = cooper.StateLogger(save_metrics=[\"loss\", \"eq_defect\", \"eq_multipliers\"])\n",
    "\n",
    "    # Run optimization\n",
    "    for iter_num in range(n_iter):\n",
    "\n",
    "        coop.zero_grad()\n",
    "        lagrangian = formulation.composite_objective(cmp.closure, x)\n",
    "        formulation.custom_backward(lagrangian)\n",
    "        coop.step(cmp.closure, x)\n",
    "\n",
    "        partial_dict = {\"params\": copy.deepcopy(x.data)}\n",
    "        state_history.store_metrics(formulation, iter_num, partial_dict)\n",
    "\n",
    "    print(cmp.state)\n",
    "\n",
    "    # # Plot the optimization metrics\n",
    "    # all_metrics = state_history.unpack_stored_metrics()\n",
    "\n",
    "    # fig, (ax0, ax1, ax2) = plt.subplots(nrows=1, ncols=3, sharex=True, figsize=(15, 3))\n",
    "\n",
    "    # ax0.plot(all_metrics[\"iters\"], np.stack(all_metrics[\"eq_multipliers\"]).squeeze())\n",
    "    # ax0.set_title(\"Multipliers\")\n",
    "\n",
    "    # ax1.plot(all_metrics[\"iters\"], np.stack(all_metrics[\"eq_defect\"]).squeeze(), alpha=0.6)\n",
    "    # ax1.axhline(0.0, c=\"gray\", alpha=0.2)\n",
    "    # ax1.set_title(\"Defects\")\n",
    "\n",
    "    # ax2.plot(all_metrics[\"iters\"], all_metrics[\"loss\"])\n",
    "    # ax2.set_title(\"Objective\")\n",
    "\n",
    "    # [_.semilogx() for _ in (ax0, ax1, ax2)]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix shape: torch.Size([5, 7])\n",
      "b vector shape: torch.Size([5, 1])\n",
      "Augmented matrix rank: 5\n",
      "Matrix rank: 5\n",
      "It has solution.\n",
      "CMPState(loss=tensor([[0.7455]], grad_fn=<MmBackward0>), ineq_defect=None, eq_defect=tensor([[ 1.9073e-06],\n",
      "        [ 3.9190e-06],\n",
      "        [-3.5763e-07],\n",
      "        [ 3.5763e-07],\n",
      "        [ 1.8775e-06]], grad_fn=<SubBackward0>), proxy_ineq_defect=None, proxy_eq_defect=None, misc=None)\n"
     ]
    }
   ],
   "source": [
    "n_vars = 7\n",
    "n_eq = 5\n",
    "scaling_value = 1\n",
    "primal_lr = 3e-2\n",
    "dual_lr = 9e-3\n",
    "n_iter = 10000\n",
    "primal_optimizer_type = \"SGD\"\n",
    "dual_optimizer_type = \"SGD\"\n",
    "\n",
    "solve_cmp(n_eq, n_vars, scaling_value, primal_lr, dual_lr, n_iter, primal_optimizer_type, dual_optimizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve an Unconstrained Problem using Cooper\n",
    "\n",
    "The dual fucntion is given by $g(v) = \\underset{x}{inf} L(x, v)$, and since the Lagrangian is a convex quadratic function we can find the minimizing x by derivating:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_x L(x, v) = 2x + A^Tv = 0 \\\\\n",
    "x = -(1/2)A^Tv\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore the dual function is:\n",
    "\n",
    "$$\n",
    "g(v) = L(-(1/2)A^Tv, v)= -(1/4)v^TAA^Tv - b^Tv\n",
    "$$\n",
    "\n",
    "Which is a concave function.\n",
    "\n",
    "Let's try to solve this unconstrained version of the dual function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 20]), torch.Size([2, 1]), tensor(True))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vars = 20\n",
    "n_eq = 2\n",
    "\n",
    "A = torch.rand(size=(n_eq,n_vars))\n",
    "b = torch.rand(size=(n_eq,1))\n",
    "\n",
    "augmented_matrix = torch.cat([A, b], dim=1)\n",
    "\n",
    "A.shape, b.shape, torch.linalg.matrix_rank(augmented_matrix) == n_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LagrangianAnalytical(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, A, b):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        super().__init__(is_constrained=False)\n",
    "\n",
    "    def closure(self, v):\n",
    "\n",
    "        vAAv = torch.matmul(torch.matmul(torch.t(v), self.A), torch.matmul(torch.t(self.A), v))\n",
    "        bv = torch.matmul(torch.t(self.b), v)\n",
    "\n",
    "        loss = (1/4)*vAAv + bv\n",
    "\n",
    "        return cooper.CMPState(loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the problem and the lagrangian formulation associated with it.\n",
    "cmp = LagrangianAnalytical(A, b)\n",
    "formulation = cooper.LagrangianFormulation(cmp)\n",
    "\n",
    "# Instantiate the variable to optimize\n",
    "v = torch.nn.Parameter(torch.rand(n_eq, 1))\n",
    "\n",
    "# Instantiate the optimizers\n",
    "primal_optimizer = torch.optim.SGD([v], lr=3e-2, momentum=0.7)\n",
    "\n",
    "# Instatiate the constrained optimizer\n",
    "coop = cooper.ConstrainedOptimizer(\n",
    "    formulation=formulation,\n",
    "    primal_optimizer=primal_optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMPState(loss=tensor([[-0.0285]], grad_fn=<AddBackward0>), ineq_defect=None, eq_defect=None, proxy_ineq_defect=None, proxy_eq_defect=None, misc=None)\n"
     ]
    }
   ],
   "source": [
    "state_history = cooper.StateLogger(save_metrics=[\"loss\"])\n",
    "\n",
    "for iter_num in range(5000):\n",
    "\n",
    "    coop.zero_grad()\n",
    "    lagrangian = formulation.composite_objective(cmp.closure, v)\n",
    "    formulation.custom_backward(lagrangian)\n",
    "    coop.step(cmp.closure, v)\n",
    "\n",
    "    # Store optimization metrics at each step\n",
    "    partial_dict = {\"params\": copy.deepcopy(v.data)}\n",
    "    state_history.store_metrics(formulation, iter_num, partial_dict)\n",
    "\n",
    "print(cmp.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_metrics = state_history.unpack_stored_metrics()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(5, 3))\n",
    "\n",
    "ax.plot(all_metrics[\"iters\"], all_metrics[\"loss\"])\n",
    "ax.set_title(\"Objective\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cooper_toy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aeb4812ab59855f52a3e65886350d5e7a32614426eb446ce24e1bf64cb4acb6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
